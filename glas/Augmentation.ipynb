{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692de406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 images in 'images/'\n",
      "165 masks in 'labels/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# First, it is necessary to separate the images in the dataset into labels and images, as in this\n",
    "# dataset all came together in the same directory\n",
    "# Furthermore, we removed the _anno.bmp from the end of the masks\n",
    "\n",
    "base_dir = 'dataset/original'\n",
    "\n",
    "images_dir = os.path.join(base_dir, \"images\")\n",
    "labels_dir = os.path.join(base_dir, \"labels\")\n",
    "\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "os.makedirs(labels_dir, exist_ok=True)\n",
    "\n",
    "bmp_files = glob.glob(os.path.join(base_dir, \"*.bmp\"))\n",
    "for bmp_path in bmp_files:\n",
    "    file_name = os.path.basename(bmp_path)\n",
    "\n",
    "    if file_name.endswith(\"_anno.bmp\"):\n",
    "        dest = os.path.join(labels_dir, file_name.replace('_anno.bmp','.bmp'))\n",
    "    else:\n",
    "        dest = os.path.join(images_dir, file_name)\n",
    "\n",
    "    shutil.copy2(bmp_path, dest)\n",
    "\n",
    "print(f\"{len(glob.glob(os.path.join(images_dir, '*.bmp')))} images in 'images/'\")\n",
    "print(f\"{len(glob.glob(os.path.join(labels_dir, '*.bmp')))} masks in 'labels/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7267644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/miniconda3/envs/pytorch5070/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in the original dataset: 165\n",
      "→ Training: 116\n",
      "→ Validation (of training): 24\n",
      "→ Test (training): 25\n",
      "\n",
      "With N=15, total images generated in training will be: 1856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/miniconda3/envs/pytorch5070/lib/python3.10/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ 0 images copied from the original valid folder.\n",
      "→ 0 images copied from the original test folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copiando valid: 100%|██████████| 24/24 [00:00<00:00, 97.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ 24 images copied from the train for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copiando test: 100%|██████████| 25/25 [00:00<00:00, 109.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ 25 images copied from the train for testing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enlarging workout images: 100%|██████████| 116/116 [00:13<00:00,  8.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final summary:\n",
      "train_images: 1856 files\n",
      "train_labels: 1856 files\n",
      "valid_images: 24 files\n",
      "valid_labels: 24 files\n",
      "test_images: 25 files\n",
      "test_labels: 25 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import config\n",
    "import sys\n",
    "import cv2\n",
    "sys.path.append('../util')\n",
    "from DatasetAugmentation import *\n",
    "\n",
    "\n",
    "#Enter the root name of the original dataset\n",
    "original_dataset_path = './dataset/original'\n",
    "output_base           = config.dataset_path\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "N = 15  # number of augmentations\n",
    "num_to_valid = 24    # number of images to move from train to valid\n",
    "num_to_test  = 25    # number of images to move from train to test\n",
    "\n",
    "\n",
    "target_size  = (256, 256)\n",
    "random.seed(42)\n",
    "\n",
    "# -----------------------------\n",
    "# Entry and exit paths\n",
    "# -----------------------------\n",
    "orig_train_img_dir  = os.path.join(original_dataset_path, 'images')\n",
    "orig_train_mask_dir = os.path.join(original_dataset_path, 'labels')\n",
    "orig_valid_img_dir  = os.path.join(original_dataset_path, 'blank')\n",
    "orig_valid_mask_dir = os.path.join(original_dataset_path, 'blank')\n",
    "orig_test_img_dir   = os.path.join(original_dataset_path, 'blank')\n",
    "orig_test_mask_dir  = os.path.join(original_dataset_path, 'blank')\n",
    "\n",
    "\n",
    "\n",
    "output_dirs = {\n",
    "    'train_images': os.path.join(output_base, 'images/train'),\n",
    "    'train_labels': os.path.join(output_base, 'labels/train'),\n",
    "    'valid_images': os.path.join(output_base, 'images/valid'),\n",
    "    'valid_labels': os.path.join(output_base, 'labels/valid'),\n",
    "    'test_images':  os.path.join(output_base, 'images/test'),\n",
    "    'test_labels':  os.path.join(output_base, 'labels/test'),\n",
    "}\n",
    "\n",
    "transforms = A.Compose([\n",
    "    A.Resize(*target_size, interpolation=cv2.INTER_NEAREST),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.7, border_mode=cv2.BORDER_REFLECT),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.ElasticTransform(p=0.2),\n",
    "    A.GaussianBlur(p=0.3),\n",
    "    A.GridDistortion(p=0.2),\n",
    "])\n",
    "\n",
    "def fix_mask(mask):\n",
    "    return (mask > 0).astype(np.uint8) * 255\n",
    "\n",
    "\n",
    "augment_dataset(N, num_to_valid, num_to_test,\n",
    "                    orig_train_img_dir, orig_train_mask_dir,\n",
    "                    orig_valid_img_dir, orig_valid_mask_dir,\n",
    "                    orig_test_img_dir, orig_test_mask_dir,\n",
    "                    output_base,\n",
    "                    transforms,\n",
    "                    function_to_apply_to_masks=fix_mask\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch5070",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
